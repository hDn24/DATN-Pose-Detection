{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pose_Detection_Estimation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOTYavBx/C7wUXspdPSerSq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hDn24/DATN-Pose-Detection/blob/feature%2Finit_source/init_source/Pose_Detection_Estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb67hOiaf4At"
      },
      "source": [
        "## Import the required libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYh7V7fxfvsl"
      },
      "source": [
        "import csv\n",
        "import cv2\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "import tqdm\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s9dJ4WMWsRz"
      },
      "source": [
        "# Load dataset into colab machine\n",
        "!gdown https://drive.google.com/uc?id=1OMCywlpMWdvnZlSxKLRAcN65wVZhYzAP -O yoga_poses.zip -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L-P6T06drYv"
      },
      "source": [
        "!unzip -q yoga_poses.zip -d '/content'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExO7LDTa46jS"
      },
      "source": [
        "# Download the model from TF Hub\n",
        "!wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/3?lite-format=tflite\n",
        "\n",
        "# Initialize a TFLite interpreter using the model downloaded\n",
        "interpreter = tf.lite.Interpreter(model_path='model.tflite')\n",
        "interpreter.allocate_tensors()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TznlF5ny473V"
      },
      "source": [
        "# Confidence score to determine whether a keypoint prediction is reliable.\n",
        "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
        "\n",
        "def init_crop_region(image_height, image_width):\n",
        "  \"\"\"Defines the default crop region.\n",
        "\n",
        "  The function provides the initial crop region (pads the full image from both\n",
        "  sides to make it a square image) when the algorithm cannot reliably determine\n",
        "  the crop region from the previous frame.\n",
        "  \"\"\"\n",
        "  if image_width > image_height:\n",
        "    box_height = image_width / image_height\n",
        "    box_width = 1.0\n",
        "    y_min = (image_height / 2 - image_width / 2) / image_height\n",
        "    x_min = 0.0\n",
        "  else:\n",
        "    box_height = 1.0\n",
        "    box_width = image_height / image_width\n",
        "    y_min = 0.0\n",
        "    x_min = (image_width / 2 - image_height / 2) / image_width\n",
        "\n",
        "  return {\n",
        "    'y_min': y_min,\n",
        "    'x_min': x_min,\n",
        "    'y_max': y_min + box_height,\n",
        "    'x_max': x_min + box_width,\n",
        "    'height': box_height,\n",
        "    'width': box_width\n",
        "  }\n",
        "\n",
        "\n",
        "def torso_visible(keypoints):\n",
        "  \"\"\"Checks whether there are enough torso keypoints.\n",
        "  \n",
        "  This function checks whether the model is confident at predicting one of the\n",
        "  shoulders/hips which is required to determine a good crop region.\n",
        "  \"\"\"\n",
        "  return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] > \n",
        "           MIN_CROP_KEYPOINT_SCORE or\n",
        "          keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] > \n",
        "           MIN_CROP_KEYPOINT_SCORE) and\n",
        "          (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] > \n",
        "           MIN_CROP_KEYPOINT_SCORE or\n",
        "          keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] > \n",
        "           MIN_CROP_KEYPOINT_SCORE))\n",
        "  \n",
        "\n",
        "def determine_torso_and_body_range(\n",
        "    keypoints, target_keypoints, center_y, center_x):\n",
        "  \"\"\"Calculates the maximum distance from each keypoints to the center location.\n",
        "  \n",
        "  The function returns the maximum distances from the two sets of keypoints:\n",
        "  full 17 keypoints and 4 torso keypoints. The returned information will be\n",
        "  used to determine the crop size. See determineCropRegion for more detail.\n",
        "  \"\"\"\n",
        "  torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
        "  max_torso_yrange = 0.0\n",
        "  max_torso_xrange = 0.0\n",
        "  for joint in torso_joints:\n",
        "    dist_y = abs(center_y - target_keypoints[joint][0])\n",
        "    dist_x = abs(center_x - target_keypoints[joint][1])\n",
        "    if dist_y > max_torso_yrange:\n",
        "      max_torso_yrange = dist_y\n",
        "    if dist_x > max_torso_xrange:\n",
        "      max_torso_xrange = dist_x\n",
        "    \n",
        "  max_body_yrange = 0.0\n",
        "  max_body_xrange = 0.0\n",
        "  for joint in KEYPOINT_DICT.keys():\n",
        "    if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
        "      continue\n",
        "    dist_y = abs(center_y - target_keypoints[joint][0]);\n",
        "    dist_x = abs(center_x - target_keypoints[joint][1]);\n",
        "    if dist_y > max_body_yrange:\n",
        "      max_body_yrange = dist_y\n",
        "    \n",
        "    if dist_x > max_body_xrange:\n",
        "      max_body_xrange = dist_x\n",
        "\n",
        "  return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]\n",
        "\n",
        "\n",
        "def determine_crop_region(\n",
        "      keypoints, image_height,\n",
        "      image_width):\n",
        "  \"\"\"Determines the region to crop the image for the model to run inference on.\n",
        "   \n",
        "  The algorithm uses the detected joints from the previous frame to estimate\n",
        "  the square region that encloses the full body of the target person and\n",
        "  centers at the midpoint of two hip joints. The crop size is determined by\n",
        "  the distances between each joints and the center point.\n",
        "  When the model is not confident with the four torso joint predictions, the\n",
        "  function returns a default crop which is the full image padded to square.\n",
        "  \"\"\"\n",
        "  target_keypoints = {}\n",
        "  for joint in KEYPOINT_DICT.keys():\n",
        "    target_keypoints[joint] = [\n",
        "      keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,\n",
        "      keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width\n",
        "    ]  \n",
        "\n",
        "  if torso_visible(keypoints):\n",
        "    center_y = (target_keypoints['left_hip'][0] + \n",
        "                target_keypoints['right_hip'][0]) / 2;\n",
        "    center_x = (target_keypoints['left_hip'][1] + \n",
        "                target_keypoints['right_hip'][1]) / 2;\n",
        "\n",
        "    (max_torso_yrange, max_torso_xrange,\n",
        "      max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n",
        "          keypoints, target_keypoints, center_y, center_x)\n",
        "      \n",
        "    crop_length_half = np.amax(\n",
        "        [max_torso_xrange * 1.9, max_torso_yrange * 1.9,\n",
        "          max_body_yrange * 1.2, max_body_xrange * 1.2])\n",
        "\n",
        "    tmp = np.array(\n",
        "        [center_x, image_width - center_x, center_y, image_height - center_y])\n",
        "    crop_length_half = np.amin(\n",
        "        [crop_length_half, np.amax(tmp)]);\n",
        "\n",
        "    crop_corner = [center_y - crop_length_half, center_x - crop_length_half];\n",
        "\n",
        "    if crop_length_half > max(image_width, image_height) / 2:\n",
        "      return init_crop_region(image_height, image_width)\n",
        "    else:\n",
        "      crop_length = crop_length_half * 2;\n",
        "      return {\n",
        "        'y_min': crop_corner[0] / image_height,\n",
        "        'x_min': crop_corner[1] / image_width,\n",
        "        'y_max': (crop_corner[0] + crop_length) / image_height,\n",
        "        'x_max': (crop_corner[1] + crop_length) / image_width,\n",
        "        'height': (crop_corner[0] + crop_length) / image_height -\n",
        "            crop_corner[0] / image_height,\n",
        "        'width': (crop_corner[1] + crop_length) / image_width -\n",
        "            crop_corner[1] / image_width\n",
        "      }\n",
        "  else:\n",
        "    return init_crop_region(image_height, image_width)\n",
        "\n",
        "\n",
        "def crop_and_resize(image, crop_region, crop_size):\n",
        "  \"\"\"Crops and resize the image to prepare for the model input.\"\"\"\n",
        "  boxes=[[crop_region['y_min'], crop_region['x_min'],\n",
        "          crop_region['y_max'], crop_region['x_max']]]\n",
        "  output_image = tf.image.crop_and_resize(\n",
        "      image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
        "  \n",
        "  return output_image\n",
        "\n",
        "\n",
        "def run_detector(interpreter, image, crop_region, crop_size):\n",
        "  \"\"\"Runs model inferece on the cropped region.\n",
        "  \n",
        "  The function runs the model inference on the cropped region and updates the\n",
        "  model output to the original image coordinate system.\n",
        "  \"\"\"\n",
        "  image_height, image_width, _ = image.shape\n",
        "  input_image = crop_and_resize(\n",
        "    tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
        "  input_image = tf.cast(input_image, dtype=tf.float32)\n",
        "\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "\n",
        "  # We use the original model for pre-processing, since the TFLite model doesn't\n",
        "  # include pre-processing.\n",
        "  interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
        "  interpreter.invoke()\n",
        "\n",
        "  keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
        "  # Update the coordinates.\n",
        "  for idx in range(17):\n",
        "    keypoints_with_scores[0, 0, idx, 0] = (\n",
        "        crop_region['y_min'] * image_height + \n",
        "        crop_region['height'] * image_height * \n",
        "        keypoints_with_scores[0, 0, idx, 0]) / image_height\n",
        "    keypoints_with_scores[0, 0, idx, 1] = (\n",
        "        crop_region['x_min'] * image_width + \n",
        "        crop_region['width'] * image_width * \n",
        "        keypoints_with_scores[0, 0, idx, 1]) / image_width\n",
        "        \n",
        "  return keypoints_with_scores\n",
        "\n",
        "def detect(interpreter, input_tensor, inference_count=3):\n",
        "  \"\"\"Run detection on an input image.\n",
        " \n",
        "  Args:\n",
        "    interpreter: tf.lite.Interpreter\n",
        "    input_tensor: A [1, height, width, 3] Tensor of type tf.float32.\n",
        "      Note that height and width can be anything since the image will be\n",
        "      immediately resized according to the needs of the model within this\n",
        "      function.\n",
        "    inference_count: Number of times the interpreter should run repeatly on the\n",
        "      same input image to improve detection accuracy.\n",
        " \n",
        "  Returns:\n",
        "    A dict containing 1 Tensor of shape [1, 1, 17, 3] representing the\n",
        "    keypoint coordinates and scores.\n",
        "  \"\"\"\n",
        "  image_height, image_width, channel = input_tensor.shape\n",
        "  # Get the input shape of the model\n",
        "  _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n",
        " \n",
        "  # Detect pose using the full input image\n",
        "  crop_region = init_crop_region(image_height, image_width)\n",
        "  keypoint_with_scores = run_detector(\n",
        "    interpreter, input_tensor, crop_region, \n",
        "    crop_size=[input_height, input_width]\n",
        "    )\n",
        " \n",
        "  # Repeatedly using previous detection result to identify the region of\n",
        "  # interest and only croping that region to improve detection accuracy\n",
        "  for _ in range(inference_count - 1):\n",
        "    crop_region = determine_crop_region(\n",
        "        keypoint_with_scores, image_height, image_width)\n",
        "    keypoint_with_scores = run_detector(\n",
        "        interpreter, input_tensor, crop_region, \n",
        "        crop_size=[input_height, input_width])\n",
        "\n",
        "  return keypoint_with_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRNPJyp87TWk"
      },
      "source": [
        "!wget -O image.jpeg https://cdn.pixabay.com/photo/2014/01/10/13/30/warrior-pose-241611_960_720.jpg\n",
        "\n",
        "image = tf.io.read_file('image.jpeg')\n",
        "image = tf.io.decode_jpeg(image)\n",
        "keypoint_with_scores = detect(interpreter, image)\n",
        "output = draw_prediction_on_image(image.numpy().astype(np.uint8),\n",
        "                                                    keypoint_with_scores, crop_region=None,\n",
        "                                                    close_figure=False, keep_input_size=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEWllUK8HPRk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}